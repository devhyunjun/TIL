- 수업 정리
spyder : web에 돌아디니면서 긁어오는 역할
1. 환경설정 
스크래핑 폴더 생성
1 1 where conda
2 scripts 디렉토리 복사/activate 실행 ->
3 pip install scrapy
4 scarpy startproject
5 cd 폴더
6 scrapy genspider GmarketBestSeller gmarket.co.kr
7 spider 폴더 밑에 spider로 이름 바꾸기 

1. item.py
모델명 처럼 scarpy해서 가져올 아이템을 클래스를 통해 정의
스크래핑 해서 가져오는 변수명과 정의하는 이름이 반드시 일치해야 함
```
# Define here the models for your scraped items
#
# See documentation in:
# https://docs.scrapy.org/en/latest/topics/items.html
import scrapy
class GmarketBestItem(scrapy.Item):
    title = scrapy.Field()  # 상품명
    s_price = scrapy.Field()  # 할인 가격
    o_price = scrapy.Field()  # 원래 가격
    discount_rate = scrapy.Field()  # 할인율
    link = scrapy.Field()  # 상세페이지링크
```


3. spider/[폴더명].py : spider폴더 하위에 init 밑에 폴더명과 동일한 py 파일, 여기서 스파이더 이름, 크롤링할 웹사이트 도메인, 주소, 설정 가능
```
class Spider(scrapy.Spider):
    # 1. 스파이더의 이름(크롤러의 이름)
    name = 'GmarketBestSellers'
    # 2. 크롤링할 웹 사이트의 도메인
    allowed_domains = ['gmarket.co.kr']
    # 3. 스크래이핑을 시작할 주소
    start_urls = ['https://corners.gmarket.co.kr/BestSellers']
    # 크롤링을 수행하여 item으로 만들어주는 메소드
    # start_urls로 요청을 보낸 결과(응답)가 들어온다.
    def parse(self, response):
        links = response.xpath(
            '//*[@id="gBestWrap"]/div/div[3]/div/ul/li/a/@href'
        ).extract()
        for link in links:
            # callback : 작업의 결과를 어떻게 처리할지를 지정
            # Request 객체를 만들어서 스파이더한테 던져줌
            yield scrapy.Request(link, callback=self.page_content)
```
여기서 parse도 정의해서 가져올 링크, 
링크에서 어떻게 작업을 처리할 건지 page_content 매소드로 추가로 정의해서 item 들을 채워
```
 def page_content(self, response):
        # 스크래이핑한 데이터를 저장할 GmarketBestItem
        item = GmarketBestItem()
        # item의 멤버 변수의 이름과 키의 이름은 항상 일치
        item["title"] = response.xpath(
            '//*[@id="itemcase_basic"]/div[1]/h1/text()'
        ).extract_first()  # extract_first() => extract()[0]
        item["s_price"] = response.xpath(
            '//*[@id="itemcase_basic"]/div[1]/p/span/strong/text()'
        ).extract_first().replace(",", "")
        try:
            item["o_price"] = response.xpath(
                '//*[@id="itemcase_basic"]/div[1]/p/span/span/text()'
            ).extract_first().replace(",", "")
        except:
            item["o_price"] = item["s_price"]
        item["discount_rate"] = str(
            round((1 - int(item["s_price"]) / int(item["o_price"]))*100, 2)) + "%"
        item["link"] = response.url  # 요청한 링크 얻기
        yield item
        
```


4. pipline : 받아온 파일을 어디로 또 어떤 형식으로 보낼 것인가를 정의 하는 파일
```
from itemadapter import ItemAdapter
import requests
import json
class GmarketBestPipeline:
    def __send_slack(self, msg):
        WEBHOOK_URL = "https://hooks.slack.com/services/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"
        payload = {
            "channel": "#잡담",
            "username": "박현준",
            "text": msg
        }
        print("payload", payload)
        requests.post(WEBHOOK_URL, json.dumps(payload))
    # 크롤링 된 아이템을 최종적으로 후처리
    def process_item(self, item, spider):
        keyword = "봄"
        if keyword in item['title']:
            msg = f"{keyword}알림! // {item['title']}, {item['s_price'],{item['link']}}"
            self.__send_slack(msg)
        return item
```


5. settings.py
pipline을 만들 경우 
```
ITEM_PIPELINES 
```
에서 주석을 해제해야 정상적으로 사용가


6. 스크래핑 한 것 내보는 법 : 1 pipline이용 2 csv 등 
scrapy crawl GmarketBestSellers -o gmarket.csv -t csv