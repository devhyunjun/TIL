{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/devhyunjun/TIL/blob/master/Treemodel_hyunjun_230520.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 데이터 불러오기"
      ],
      "metadata": {
        "id": "Gr0dzicqafo_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # colab 나눔고딕 모듈\n",
        "# !apt-get update -qq\n",
        "# !apt-get install fonts-nanum* -qq\n",
        "# !pip install pymysql\n",
        "\n",
        "# 폰트 설정\n",
        "font_path = '/content\\fonts/NanumGothic.ttf'\n",
        "import matplotlib.pyplot as plt\n",
        "# font_name = plt.font_manager.fontManager.FontProperties(fname=font_path).get_name()\n",
        "plt.rc('font', family=font_name)\n",
        "\n",
        "# 경고 메시지 무시\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=plt.font_manager.fontManager.FontManagerWarning)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Z0V3IYNtcYB",
        "outputId": "9162c645-c18a-4642-f571-cf28929748c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-0c09576a84e6>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mfont_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content\\fonts/NanumGothic.ttf'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mfont_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfont_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfontManager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFontProperties\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfont_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'font'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfamily\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfont_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'matplotlib.pyplot' has no attribute 'font_manager'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "veBR4-_9sxyM"
      },
      "outputs": [],
      "source": [
        "import pymysql\n",
        "\n",
        "#for 시각화\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "\n",
        "# 검증\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import KFold,cross_validate\n",
        "from sklearn.metrics import make_scorer, mean_squared_error, mean_absolute_error, mean_squared_log_error, r2_score\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "# 모델import\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from lightgbm import LGBMRegressor\n",
        "from sklearn.ensemble import GradientBoostingRegressor"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install bayesian-optimization"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jaG_aiLo-O2p",
        "outputId": "d3b1b84a-dacd-486d-a329-66d1583708f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting bayesian-optimization\n",
            "  Downloading bayesian_optimization-1.4.3-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from bayesian-optimization) (1.22.4)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from bayesian-optimization) (1.10.1)\n",
            "Requirement already satisfied: scikit-learn>=0.18.0 in /usr/local/lib/python3.10/dist-packages (from bayesian-optimization) (1.2.2)\n",
            "Collecting colorama>=0.4.6 (from bayesian-optimization)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.18.0->bayesian-optimization) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.18.0->bayesian-optimization) (3.1.0)\n",
            "Installing collected packages: colorama, bayesian-optimization\n",
            "Successfully installed bayesian-optimization-1.4.3 colorama-0.4.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LSjt3BaRaESE"
      },
      "outputs": [],
      "source": [
        "def get_room_direction_score(row):\n",
        "    if row['room_direction_text'] == '남향':\n",
        "        return 3\n",
        "    elif row['room_direction_text'] in ['남동향', '남서향']:\n",
        "        return 2\n",
        "    elif row['room_direction_text'] in ['동향', '서향']:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "def get_floor_score(row):\n",
        "    if row['_floor'] == 1:\n",
        "        return 0\n",
        "    elif row['_floor']==2:\n",
        "        return 1\n",
        "    else:\n",
        "        return 2\n",
        "\n",
        "def get_ev_score(row):\n",
        "    if (row['_floor'] >= 3) & (row['elevator'] == 0) :\n",
        "        return 0\n",
        "    else:\n",
        "        return 1\n",
        "\n",
        "def get_finaldf():\n",
        "  db= pymysql.connect(\n",
        "\n",
        "  # cursor = conn.cursor()\n",
        "  df = pd.read_sql(\"SELECT * FROM zb_final\", db, index_col='id')\n",
        "  # result = cursor.fetchall()\n",
        "  # for record in result:\n",
        "  #     print(record)\n",
        "  db.close()\n",
        "\n",
        "  df['rent_adjusted'] = df['deposit']*0.05/12 + df['rent'] + df['manage_cost']\n",
        "\n",
        "  # 창 반향 전환\n",
        "  df['room_direction_score'] = df.apply(get_room_direction_score, axis=1)\n",
        "\n",
        "  # 옵션 갯수, 지하철 수 숫자로 변환\n",
        "  df['manage_cost_inc_num'] = df['manage_cost_inc'].str.split(',').apply(len)\n",
        "  df['near_subways_num'] = df['near_subways'].str.split(',').apply(len)\n",
        "\n",
        "  # 매물 변수 drop\n",
        "  df.drop('manage_cost_inc', axis=1, inplace=True)\n",
        "  df.drop('near_subways', axis=1, inplace=True)\n",
        "  df.drop('options', axis=1, inplace=True)\n",
        "  # 읍면동 주거유형 갯수에서 비율로\n",
        "  df['tenure_self_ratio'] = df['tenure_self'] / df['tenure_total']\n",
        "  df['tenure_jeonse_ratio'] = df['tenure_jeonse'] / df['tenure_total']\n",
        "  df['tenure_free_ratio'] = df['tenure_free'] / df['tenure_total']\n",
        "  df['tenure_monthly_ratio'] = df['tenure_monthly'] / df['tenure_total']\n",
        "\n",
        "  # 주거유형 갯수 drop\n",
        "  df.drop(['tenure_self', 'tenure_jeonse', 'tenure_free', 'tenure_monthly'], axis=1, inplace=True)\n",
        "\n",
        "  # 층 점수화\n",
        "  df['room_floor_score'] = df.apply(get_floor_score, axis=1)\n",
        "\n",
        "  # 엘리베이터 점수화\n",
        "  df['get_ev_score'] = df.apply(get_ev_score, axis=1)\n",
        "\n",
        "  # 문자열, 코드 등 회귀변수 안쓰는 변수 drop\n",
        "  drop_columns = ['address1', 'address2', '_floor' , 'room_direction_text', 'images',\n",
        "          'description', 'title', 'add1', 'add2', 'add3', 'sgg_cd',\n",
        "          'emd_cd_2022',\n",
        "          'emd_cd_2020',\n",
        "          'sido_nm',\n",
        "          'sgg_nm',\n",
        "          'emd_nm', \n",
        "          'building_total',\n",
        "          'hhd_total'\n",
        "          ]\n",
        "  # 안쓰는 변수 drop 된 df 정의\n",
        "  df =  df.drop(drop_columns, axis=1, inplace=False)\n",
        "  # 관리비 100 이상 말이 안됨 -> 원세랑 비교\n",
        "  df[df['manage_cost']>100]\n",
        "  df=df.drop(df[df['manage_cost']>100].index)\n",
        "\n",
        "  df[df['rent']>1000]\n",
        "  df=df.drop(df[df['rent']>1000].index)\n",
        "\n",
        "  df[df['size_m2']>150]\n",
        "  df=df.drop(df[df['size_m2']>150].index)\n",
        "  df=df.drop(df[(df['service_type']=='원룸')&(df['size_m2']>120)].index)\n",
        "\n",
        "  # df[(df['service_type']=='원룸')&(df['size']>40)]\n",
        "  df=df.drop(df[(df['service_type']=='원룸')&(df['size_m2']>99)].index)\n",
        "\n",
        "  df=df.drop(df[(df['service_type']=='원룸')&(df['manage_cost']>50)].index)\n",
        "  df = pd.get_dummies(df)\n",
        "\n",
        "  df.drop(['deposit', 'rent','school_dist', 'elevator', 'manage_cost'], axis=1, inplace=True)\n",
        "  return df\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = get_finaldf()\n",
        "df.head(5)\n",
        "df.info()"
      ],
      "metadata": {
        "id": "vIuXLWAjzcGd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YA5ak1GPnp41"
      },
      "outputs": [],
      "source": [
        "# # 훈련 데이터에 대한 MSE 계산\n",
        "# mse_train = mean_squared_error(y_train, y_pred_train)\n",
        "# print(\"Train RMSE:\", np.sqrt(abs(mse_train)))\n",
        "\n",
        "# # 적합된 모델을 사용하여 테스트 데이터에 대한 예측 수행\n",
        "# y_pred_test = modelLGBM.predict(X_poly_2_test)\n",
        "\n",
        "# # 테스트 데이터에 대한 MSE 계산\n",
        "# mse_test = mean_squared_error(y_test, y_pred_test)\n",
        "# print(\"Test RMSE:\", np.sqrt(abs(mse_test)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LPlt_RDMnp40"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import mean_squared_error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {
        "id": "9tgz9Lmvnp4z"
      },
      "outputs": [],
      "source": [
        "df_test = pd.get_dummies(df)\n",
        "\n",
        "X = df_test.drop(['rent_adjusted'],axis=1)\n",
        "y = df_test['rent_adjusted']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42, shuffle=True )"
      ],
      "metadata": {
        "id": "JB8N8NwQAvTH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 데이터분포 확인"
      ],
      "metadata": {
        "id": "_a9SduLU6ENU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 집값\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "sns.kdeplot(df_test['rent_adjusted'])\n",
        "plt.xlabel('Value')\n",
        "plt.ylabel('Density')\n",
        "plt.title('rent_adjusted Kernel Density Estimation')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hVQB3R_E2aNg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 경고 메시지 무시\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "sns.kdeplot(df_test['rent_adjusted'])\n",
        "plt.xlabel('Value')\n",
        "plt.ylabel('Density')\n",
        "plt.title('rent_adjusted Kernel Density Estimation')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vKt4IRdP6CyV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbsKqWqwnp41"
      },
      "source": [
        "# Tree Model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LightGBM"
      ],
      "metadata": {
        "id": "hA-j51oyasY0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 원본데이터"
      ],
      "metadata": {
        "id": "ndnBHHOlv9uW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "modelLGBM = LGBMRegressor(n_jobs=-1)\n",
        "\n",
        "print('BASELINE -- original features')\n",
        "print(len(X_train), len(X_test))\n",
        "modelLGBM.fit(X_train, y_train)\n",
        "\n",
        "y_pred_train = modelLGBM.predict(X_train)\n",
        "\n",
        "# 훈련 데이터에 대한 MSE 계산\n",
        "mse_train = mean_squared_error(y_train, y_pred_train)\n",
        "print(\"Train RMSE:\", np.sqrt(abs(mse_train)))\n",
        "\n",
        "# 적합된 모델을 사용하여 테스트 데이터에 대한 예측 수행\n",
        "y_pred_test = modelLGBM.predict(X_test)\n",
        "\n",
        "# 테스트 데이터에 대한 MSE 계산\n",
        "mse_test = mean_squared_error(y_pred_test, y_test)\n",
        "print(\"Test RMSE:\", np.sqrt(abs(mse_test)))\n",
        "print(\"Train, Test 차이 비율\", (np.sqrt(abs(mse_test))- np.sqrt(abs(mse_train)))/(np.sqrt(abs(mse_train))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2j3gXci2dOZz",
        "outputId": "d934e1bc-9505-4bfa-bcb9-093f6d779989"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BASELINE -- original features\n",
            "25656 8552\n",
            "Train RMSE: 14.376610525475263\n",
            "Test RMSE: 17.599272073149464\n",
            "Train, Test 차이 비율 0.22416003702428092\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ORIGINAL DATA -  finding best parameter by using baseian method"
      ],
      "metadata": {
        "id": "EjdpSMbu_cy7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import lightgbm as lgb\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from bayes_opt import BayesianOptimization\n",
        "\n",
        "# Define the loss function to be optimized\n",
        "def loss_function(learning_rate, num_leaves, max_depth, n_estimators, min_child_samples):\n",
        "    params = {\n",
        "        'objective': 'regression',\n",
        "        'metric': 'mse',\n",
        "        'learning_rate': learning_rate,\n",
        "        'num_leaves': int(num_leaves),\n",
        "        'max_depth': int(max_depth),\n",
        "        'n_estimators' : int(n_estimators),\n",
        "        'min_child_samples' : int(min_child_samples)\n",
        "    }\n",
        "    # Train the LightGBM model with the specified parameters\n",
        "    model = lgb.LGBMRegressor(**params)\n",
        "    model.fit(X_train, y_train)\n",
        "    \n",
        "    # Make predictions on the validation set\n",
        "    y_pred = model.predict(X_val)\n",
        "    \n",
        "    # Calculate the mean squared error (MSE)\n",
        "    mse = mean_squared_error(y_val, y_pred)\n",
        "    \n",
        "    # Return the negative MSE as the loss to be minimized\n",
        "    return -mse\n",
        "\n",
        "# Define the parameter ranges for the optimizer 파라미터 범위\n",
        "# 일반적으로 러닝레이트가 낮고, max\n",
        "pbounds = {'learning_rate': (0.01, 0.15),\n",
        "           'num_leaves': (10, 50),\n",
        "           'max_depth': (3, 15),\n",
        "           'n_estimators' : (100, 300),\n",
        "           'min_child_samples': (20,40)}\n",
        "\n",
        "# Create the Bayesian optimizer\n",
        "optimizer = BayesianOptimization(f=loss_function, pbounds=pbounds, random_state=42)\n",
        "\n",
        "# Run the optimization\n",
        "#init_point : 최적화를 진행하기 전 초기에 랜덤하게 점 뿌려주는 갯수,\n",
        "# 탐색과 활용을 균형있게 고려하기 위해 초기 점의 개수를 작은 값으로 시작하고 (예: 5-10),\n",
        "#  적절한 반복 횟수를 선택하는 것이 좋습니다 (예: 10-20) 조정\n",
        "optimizer.maximize(init_points=5, n_iter=12)\n",
        "\n",
        "# Get the best parameters and the best loss achieved\n",
        "best_params = optimizer.max['params']\n",
        "best_loss = -optimizer.max['target']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F-loGYUw_yn8",
        "outputId": "f991dfd6-164f-4062-f780-511b0b2e80b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "|   iter    |  target   | learni... | max_depth | min_ch... | n_esti... | num_le... |\n",
            "-------------------------------------------------------------------------------------\n",
            "| \u001b[0m1        \u001b[0m | \u001b[0m-318.3   \u001b[0m | \u001b[0m0.06244  \u001b[0m | \u001b[0m14.41    \u001b[0m | \u001b[0m34.64    \u001b[0m | \u001b[0m219.7    \u001b[0m | \u001b[0m16.24    \u001b[0m |\n",
            "| \u001b[0m2        \u001b[0m | \u001b[0m-435.9   \u001b[0m | \u001b[0m0.03184  \u001b[0m | \u001b[0m3.697    \u001b[0m | \u001b[0m37.32    \u001b[0m | \u001b[0m220.2    \u001b[0m | \u001b[0m38.32    \u001b[0m |\n",
            "| \u001b[0m3        \u001b[0m | \u001b[0m-537.5   \u001b[0m | \u001b[0m0.01288  \u001b[0m | \u001b[0m14.64    \u001b[0m | \u001b[0m36.65    \u001b[0m | \u001b[0m142.5    \u001b[0m | \u001b[0m17.27    \u001b[0m |\n",
            "| \u001b[0m4        \u001b[0m | \u001b[0m-348.9   \u001b[0m | \u001b[0m0.03568  \u001b[0m | \u001b[0m6.651    \u001b[0m | \u001b[0m30.5     \u001b[0m | \u001b[0m186.4    \u001b[0m | \u001b[0m21.65    \u001b[0m |\n",
            "| \u001b[0m5        \u001b[0m | \u001b[0m-344.7   \u001b[0m | \u001b[0m0.09566  \u001b[0m | \u001b[0m4.674    \u001b[0m | \u001b[0m25.84    \u001b[0m | \u001b[0m173.3    \u001b[0m | \u001b[0m28.24    \u001b[0m |\n",
            "| \u001b[95m6        \u001b[0m | \u001b[95m-313.6   \u001b[0m | \u001b[95m0.15     \u001b[0m | \u001b[95m15.0     \u001b[0m | \u001b[95m24.71    \u001b[0m | \u001b[95m206.8    \u001b[0m | \u001b[95m10.0     \u001b[0m |\n",
            "| \u001b[0m7        \u001b[0m | \u001b[0m-379.8   \u001b[0m | \u001b[0m0.01959  \u001b[0m | \u001b[0m14.85    \u001b[0m | \u001b[0m35.23    \u001b[0m | \u001b[0m218.6    \u001b[0m | \u001b[0m18.13    \u001b[0m |\n",
            "| \u001b[0m8        \u001b[0m | \u001b[0m-356.8   \u001b[0m | \u001b[0m0.04746  \u001b[0m | \u001b[0m13.7     \u001b[0m | \u001b[0m26.25    \u001b[0m | \u001b[0m204.7    \u001b[0m | \u001b[0m11.72    \u001b[0m |\n",
            "| \u001b[0m9        \u001b[0m | \u001b[0m-336.1   \u001b[0m | \u001b[0m0.06538  \u001b[0m | \u001b[0m14.54    \u001b[0m | \u001b[0m33.59    \u001b[0m | \u001b[0m221.3    \u001b[0m | \u001b[0m12.72    \u001b[0m |\n",
            "| \u001b[0m10       \u001b[0m | \u001b[0m-453.4   \u001b[0m | \u001b[0m0.01581  \u001b[0m | \u001b[0m14.4     \u001b[0m | \u001b[0m24.92    \u001b[0m | \u001b[0m212.0    \u001b[0m | \u001b[0m10.21    \u001b[0m |\n",
            "| \u001b[0m11       \u001b[0m | \u001b[0m-325.6   \u001b[0m | \u001b[0m0.09422  \u001b[0m | \u001b[0m14.88    \u001b[0m | \u001b[0m36.71    \u001b[0m | \u001b[0m218.3    \u001b[0m | \u001b[0m12.6     \u001b[0m |\n",
            "| \u001b[0m12       \u001b[0m | \u001b[0m-319.8   \u001b[0m | \u001b[0m0.1028   \u001b[0m | \u001b[0m10.68    \u001b[0m | \u001b[0m35.73    \u001b[0m | \u001b[0m220.6    \u001b[0m | \u001b[0m11.97    \u001b[0m |\n",
            "| \u001b[95m13       \u001b[0m | \u001b[95m-298.4   \u001b[0m | \u001b[95m0.1332   \u001b[0m | \u001b[95m11.87    \u001b[0m | \u001b[95m36.93    \u001b[0m | \u001b[95m221.6    \u001b[0m | \u001b[95m15.09    \u001b[0m |\n",
            "| \u001b[0m14       \u001b[0m | \u001b[0m-301.3   \u001b[0m | \u001b[0m0.1068   \u001b[0m | \u001b[0m8.896    \u001b[0m | \u001b[0m33.4     \u001b[0m | \u001b[0m225.3    \u001b[0m | \u001b[0m16.54    \u001b[0m |\n",
            "| \u001b[95m15       \u001b[0m | \u001b[95m-294.5   \u001b[0m | \u001b[95m0.15     \u001b[0m | \u001b[95m7.901    \u001b[0m | \u001b[95m33.32    \u001b[0m | \u001b[95m220.3    \u001b[0m | \u001b[95m16.66    \u001b[0m |\n",
            "| \u001b[0m16       \u001b[0m | \u001b[0m-304.4   \u001b[0m | \u001b[0m0.15     \u001b[0m | \u001b[0m5.482    \u001b[0m | \u001b[0m37.78    \u001b[0m | \u001b[0m223.0    \u001b[0m | \u001b[0m16.32    \u001b[0m |\n",
            "| \u001b[0m17       \u001b[0m | \u001b[0m-392.7   \u001b[0m | \u001b[0m0.07254  \u001b[0m | \u001b[0m3.854    \u001b[0m | \u001b[0m33.78    \u001b[0m | \u001b[0m223.1    \u001b[0m | \u001b[0m20.27    \u001b[0m |\n",
            "=====================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = lgb.LGBMRegressor(**{'learning_rate': 0.1,\n",
        " 'max_depth': -1,\n",
        " 'min_child_samples': 40,\n",
        " 'n_estimators': 300,\n",
        " 'num_leaves': 20})\n",
        "model.fit(X_train, y_train)\n",
        "y_pred_train = model.predict(X_train)\n",
        "\n",
        "# 훈련 데이터에 대한 MSE 계산\n",
        "mse_train = mean_squared_error(y_train, y_pred_train)\n",
        "print(\"Train RMSE:\", np.sqrt(abs(mse_train)))\n",
        "\n",
        "# 적합된 모델을 사용하여 테스트 데이터에 대한 예측 수행\n",
        "y_pred_val = model.predict(X_val)\n",
        "\n",
        "# 테스트 데이터에 대한 MSE 계산\n",
        "mse_test = mean_squared_error(y_val, y_pred_val)\n",
        "print(\"Test RMSE:\", np.sqrt(abs(mse_test)))\n",
        "print(\"Train, Test 차이 비율\", (np.sqrt(abs(mse_test))- np.sqrt(abs(mse_train)))/(np.sqrt(abs(mse_train))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Etn1VOMNmHJ",
        "outputId": "a38fb230-8bf6-4178-cd83-a0e6c8794271"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train RMSE: 12.809444731510853\n",
            "Test RMSE: 16.901648255082126\n",
            "Train, Test 차이 비율 0.31946767477785937\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 베스트 파라미터 TEST SET 검증"
      ],
      "metadata": {
        "id": "THhNrCQEYzbF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = lgb.LGBMRegressor(**{'learning_rate': 0.1,\n",
        " 'max_depth': -1,\n",
        " 'min_child_samples': 40,\n",
        " 'n_estimators': 300,\n",
        " 'num_leaves': 20})\n",
        "model.fit(X_train, y_train)\n",
        "y_pred_train = model.predict(X_train)\n",
        "\n",
        "# 훈련 데이터에 대한 MSE 계산\n",
        "mse_train = mean_squared_error(y_train, y_pred_train)\n",
        "print(\"Train RMSE:\", np.sqrt(abs(mse_train)))\n",
        "\n",
        "# 적합된 모델을 사용하여 테스트 데이터에 대한 예측 수행\n",
        "y_pred_test = model.predict(X_test)\n",
        "\n",
        "# 테스트 데이터에 대한 MSE 계산\n",
        "mse_test = mean_squared_error(y_test, y_pred_test)\n",
        "print(\"Test RMSE:\", np.sqrt(abs(mse_test)))\n",
        "print(\"Train, Test 차이 비율\", (np.sqrt(abs(mse_test))- np.sqrt(abs(mse_train)))/(np.sqrt(abs(mse_train))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 396
        },
        "outputId": "2a25140f-fdc3-4b25-a6e5-43dc67a033a9",
        "id": "ZV7xdeX5YzbR"
      },
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-136-50fa0bc39a8b>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m  \u001b[0;34m'n_estimators'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m  'num_leaves': 20})\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0my_pred_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_init_score, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[1;32m    893\u001b[0m             callbacks=None, init_model=None):\n\u001b[1;32m    894\u001b[0m         \u001b[0;34m\"\"\"Docstring is inherited from the LGBMModel.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         super().fit(X, y, sample_weight=sample_weight, init_score=init_score,\n\u001b[0m\u001b[1;32m    896\u001b[0m                     \u001b[0meval_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_sample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_sample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                     \u001b[0meval_init_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_init_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_metric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_metric\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[1;32m    746\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord_evaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    747\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 748\u001b[0;31m         self._Booster = train(\n\u001b[0m\u001b[1;32m    749\u001b[0m             \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m             \u001b[0mtrain_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightgbm/engine.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    290\u001b[0m                                     evaluation_result_list=None))\n\u001b[1;32m    291\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m         \u001b[0mbooster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0mevaluation_result_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, train_set, fobj)\u001b[0m\n\u001b[1;32m   3019\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__set_objective_to_none\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3020\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mLightGBMError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Cannot update due to null objective function.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3021\u001b[0;31m             _safe_call(_LIB.LGBM_BoosterUpdateOneIter(\n\u001b[0m\u001b[1;32m   3022\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3023\u001b[0m                 ctypes.byref(is_finished)))\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*### vs polynomial 비교"
      ],
      "metadata": {
        "id": "WBC_ZFovwmES"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 로그변환"
      ],
      "metadata": {
        "id": "jYz6-I09Xs8g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "temp_col = ['building_nonresidential_p',\n",
        " 'corp_to_pop',\n",
        " 'building_others_p',\n",
        " 'medical_per_ppltn',\n",
        " 'subway_dist',\n",
        " 'cultural_venue_dist',\n",
        " 'hhd_collective_p',\n",
        " 'convenience_per_ppltn',\n",
        " 'culture_per_ppltn',\n",
        " 'hhd_private_p',\n",
        " 'green_per_area',\n",
        " 'ppltn_foreign_domestic_ratio',\n",
        " 'convenience_store_dist',\n",
        " 'hospital_dist',\n",
        " 'park_per_area',\n",
        " 'building_yeonlip_p',\n",
        " 'grocery_per_ppltn',\n",
        " 'tenure_free_ratio',\n",
        " 'supermarket_dist',\n",
        " 'gender_ratio',\n",
        " 'gym_per_ppltn',\n",
        " 'building_dandok_p',\n",
        " 'shopping_per_ppltn',\n",
        " 'public_institution_dist',\n",
        " 'size_m2',\n",
        " 'restaurant_per_ppltn',\n",
        " 'ppltn_adult_p',\n",
        " 'ppltn_total']\n",
        "\n",
        "temp_X_train = X_train.copy()\n",
        "temp_X_test = X_test.copy()\n",
        "temp2_X_train = X_train.copy()\n",
        "temp2_X_test = X_test.copy()\n",
        "\n",
        "scaler = StandardScaler()\n",
        "temp2_X_train_scaled = scaler.fit_transform(temp2_X_train)\n",
        "temp2_X_test_scaled = scaler.fit_transform(temp2_X_test)\n",
        "X_train_scaled = pd.DataFrame(temp2_X_train_scaled)\n",
        "X_test_scaled = pd.DataFrame(temp2_X_test_scaled)\n",
        "\n",
        "\n",
        "log_train= np.log(temp_X_train[temp_col])\n",
        "log_test= np.log(temp_X_test[temp_col])\n",
        "X_train_scaled[temp_col] = log_train\n",
        "X_test_scaled[temp_col] = log_test\n",
        "\n",
        "\n",
        "temp_X_train = temp_X_train[np.isfinite(X_train_scaled)]\n",
        "temp_X_test = temp_X_test[np.isfinite(X_test_scaled)]"
      ],
      "metadata": {
        "id": "twRBT8dXXyO5"
      },
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {
        "outputId": "f5d95db4-332b-437c-e6de-bba633f548d7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gb8b6J7Qt7ep"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BASELINE -- original features\n",
            "25656 8552\n",
            "Train RMSE: 43.91372483988172\n",
            "Test RMSE: 46.4663699473931\n",
            "Train, Test 차이 비율 0.05812864011920719\n"
          ]
        }
      ],
      "source": [
        "modelLGBM = LGBMRegressor(n_jobs=-1)\n",
        "\n",
        "print('BASELINE -- original features')\n",
        "print(len(temp_X_train), len(temp_X_test))\n",
        "modelLGBM.fit(temp_X_train, y_train)\n",
        "\n",
        "y_pred_train = modelLGBM.predict(temp_X_train)\n",
        "\n",
        "# 훈련 데이터에 대한 MSE 계산\n",
        "mse_train = mean_squared_error(y_train, y_pred_train)\n",
        "print(\"Train RMSE:\", np.sqrt(abs(mse_train)))\n",
        "\n",
        "# 적합된 모델을 사용하여 테스트 데이터에 대한 예측 수행\n",
        "y_pred_test = modelLGBM.predict(temp_X_test)\n",
        "\n",
        "# 테스트 데이터에 대한 MSE 계산\n",
        "mse_test = mean_squared_error(y_pred_test, y_test)\n",
        "print(\"Test RMSE:\", np.sqrt(abs(mse_test)))\n",
        "print(\"Train, Test 차이 비율\", (np.sqrt(abs(mse_test))- np.sqrt(abs(mse_train)))/(np.sqrt(abs(mse_train))))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### POLY"
      ],
      "metadata": {
        "id": "nwNEgY25ea63"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remind that you are using  train, validation and test sets for analysis\n",
        "# X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42, shuffle=True )"
      ],
      "metadata": {
        "id": "AlXMP3CTRlUb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iuuXJ60Znp40"
      },
      "outputs": [],
      "source": [
        "poly = PolynomialFeatures(degree=2, include_bias=True)\n",
        "X_train_poly = poly.fit_transform(X_train)\n",
        "X_val_poly = poly.transform(X_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Finding Best params by bayseian"
      ],
      "metadata": {
        "id": "NujHEy1uRYvJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import lightgbm as lgb\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from bayes_opt import BayesianOptimization\n",
        "\n",
        "# Define the loss function to be optimized\n",
        "def loss_function(learning_rate, num_leaves, max_depth, n_estimators, min_child_samples):\n",
        "    params = {\n",
        "        'objective': 'regression',\n",
        "        'metric': 'mse',\n",
        "        'learning_rate': learning_rate,\n",
        "        'num_leaves': int(num_leaves),\n",
        "        'max_depth': int(max_depth),\n",
        "        'n_estimators' : int(n_estimators),\n",
        "        'min_child_samples' : int(min_child_samples)\n",
        "    }\n",
        "    # Train the LightGBM model with the specified parameters\n",
        "    model = lgb.LGBMRegressor(**params)\n",
        "    model.fit(X_train_poly, y_train)\n",
        "    \n",
        "    # Make predictions on the validation set\n",
        "    y_pred = model.predict(X_val_poly)\n",
        "    \n",
        "    # Calculate the mean squared error (MSE)\n",
        "    mse = mean_squared_error(y_val, y_pred)\n",
        "    \n",
        "    # Return the negative MSE as the loss to be minimized\n",
        "    return -mse\n",
        "\n",
        "# Define the parameter ranges for the optimizer 파라미터 범위\n",
        "# 일반적으로 러닝레이트가 낮고, max\n",
        "pbounds = {'learning_rate': (0.01, 0.1),\n",
        "           'num_leaves': (10, 50),\n",
        "           'max_depth': (3, 10),\n",
        "           'n_estimators' : (100, 300),\n",
        "           'min_child_samples' : (20,40)\n",
        "           }\n",
        "\n",
        "# Create the Bayesian optimizer\n",
        "optimizer = BayesianOptimization(f=loss_function, pbounds=pbounds, random_state=42)\n",
        "\n",
        "# Run the optimization\n",
        "#init_point : 최적화를 진행하기 전 초기에 랜덤하게 점 뿌려주는 갯수,\n",
        "# 탐색과 활용을 균형있게 고려하기 위해 초기 점의 개수를 작은 값으로 시작하고 (예: 5-10),\n",
        "#  적절한 반복 횟수를 선택하는 것이 좋습니다 (예: 10-20) 조정\n",
        "optimizer.maximize(init_points=5, n_iter=12)\n",
        "\n",
        "# Get the best parameters and the best loss achieved\n",
        "best_params = optimizer.max['params']\n",
        "best_loss = -optimizer.max['target']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        },
        "outputId": "44562732-0095-46ef-b049-61b087a94280",
        "id": "n4igsCIhRT66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "|   iter    |  target   | learni... | max_depth | min_ch... | n_esti... | num_le... |\n",
            "-------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-81-d078708e8c24>\u001b[0m in \u001b[0;36m<cell line: 45>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;31m# 탐색과 활용을 균형있게 고려하기 위해 초기 점의 개수를 작은 값으로 시작하고 (예: 5-10),\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;31m#  적절한 반복 횟수를 선택하는 것이 좋습니다 (예: 10-20) 조정\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaximize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_points\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;31m# Get the best parameters and the best loss achieved\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/bayes_opt/bayesian_optimization.py\u001b[0m in \u001b[0;36mmaximize\u001b[0;34m(self, init_points, n_iter, acquisition_function, acq, kappa, kappa_decay, kappa_decay_delay, xi, **gp_params)\u001b[0m\n\u001b[1;32m    308\u001b[0m                 \u001b[0mx_probe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuggest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m                 \u001b[0miteration\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 310\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprobe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_probe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlazy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bounds_transformer\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0miteration\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/bayes_opt/bayesian_optimization.py\u001b[0m in \u001b[0;36mprobe\u001b[0;34m(self, params, lazy)\u001b[0m\n\u001b[1;32m    206\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprobe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOPTIMIZATION_STEP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/bayes_opt/target_space.py\u001b[0m in \u001b[0;36mprobe\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m    234\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_as_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m         \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constraint\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-81-d078708e8c24>\u001b[0m in \u001b[0;36mloss_function\u001b[0;34m(learning_rate, num_leaves, max_depth, n_estimators, min_child_samples)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# Train the LightGBM model with the specified parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLGBMRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_poly\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# Make predictions on the validation set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_init_score, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[1;32m    893\u001b[0m             callbacks=None, init_model=None):\n\u001b[1;32m    894\u001b[0m         \u001b[0;34m\"\"\"Docstring is inherited from the LGBMModel.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         super().fit(X, y, sample_weight=sample_weight, init_score=init_score,\n\u001b[0m\u001b[1;32m    896\u001b[0m                     \u001b[0meval_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_sample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_sample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                     \u001b[0meval_init_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_init_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_metric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_metric\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[1;32m    746\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord_evaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    747\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 748\u001b[0;31m         self._Booster = train(\n\u001b[0m\u001b[1;32m    749\u001b[0m             \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m             \u001b[0mtrain_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightgbm/engine.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    290\u001b[0m                                     evaluation_result_list=None))\n\u001b[1;32m    291\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m         \u001b[0mbooster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0mevaluation_result_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, train_set, fobj)\u001b[0m\n\u001b[1;32m   3019\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__set_objective_to_none\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3020\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mLightGBMError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Cannot update due to null objective function.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3021\u001b[0;31m             _safe_call(_LIB.LGBM_BoosterUpdateOneIter(\n\u001b[0m\u001b[1;32m   3022\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3023\u001b[0m                 ctypes.byref(is_finished)))\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "outputId": "d12522dc-c8cd-42ed-ee16-1263fa855ddd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RF768QOvv8Z0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BASELINE -- polynomial features\n",
            "19242 6414\n",
            "Train RMSE: 12.192004916352772\n",
            "Test RMSE: 17.22823689781495\n",
            "Train, Test 차이 비율 0.4130766035623254\n"
          ]
        }
      ],
      "source": [
        "modelLGBM = LGBMRegressor(n_jobs=-1, min_child_samples=30)\n",
        "\n",
        "print('BASELINE -- original features')\n",
        "print(len(X_train_poly), len(X_val_poly))\n",
        "modelLGBM.fit(X_train_poly, y_train)\n",
        "\n",
        "y_pred_train = modelLGBM.predict(X_train_poly)\n",
        "\n",
        "# 훈련 데이터에 대한 MSE 계산\n",
        "mse_train = mean_squared_error(y_train, y_pred_train)\n",
        "print(\"Train RMSE:\", np.sqrt(abs(mse_train)))\n",
        "\n",
        "# 적합된 모델을 사용하여 테스트 데이터에 대한 예측 수행\n",
        "y_pred_test = modelLGBM.predict(X_val_poly)\n",
        "\n",
        "# 테스트 데이터에 대한 MSE 계산\n",
        "mse_test = mean_squared_error(y_val, y_pred_val)\n",
        "print(\"Test RMSE:\", np.sqrt(abs(mse_test)))\n",
        "print(\"Train, Test 차이 비율\", (np.sqrt(abs(mse_test))- np.sqrt(abs(mse_train)))/(np.sqrt(abs(mse_train))))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_params"
      ],
      "metadata": {
        "id": "8yU-7FSYRT68"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### POLY 베스트 파라미터 TEST SET 검증"
      ],
      "metadata": {
        "id": "OAZzHjAwRT68"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model = lgb.LGBMRegressor(**{'learning_rate': 0.1,\n",
        " 'max_depth': 10,\n",
        " 'n_estimators': 243,\n",
        " 'num_leaves': 28})\n",
        "model.fit(X_train, y_train)\n",
        "y_pred_train = model.predict(X_train)\n",
        "\n",
        "# 훈련 데이터에 대한 MSE 계산\n",
        "mse_train = mean_squared_error(y_train, y_pred_train)\n",
        "print(\"Train RMSE:\", np.sqrt(abs(mse_train)))\n",
        "\n",
        "# 적합된 모델을 사용하여 테스트 데이터에 대한 예측 수행\n",
        "y_pred_test = model.predict(X_test)\n",
        "\n",
        "# 테스트 데이터에 대한 MSE 계산\n",
        "mse_test = mean_squared_error(y_test, y_pred_test)\n",
        "print(\"Test RMSE:\", np.sqrt(abs(mse_test)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 405
        },
        "outputId": "054859ce-9eaf-4e77-bda8-a2170cbede2f",
        "id": "q-a3fW3yRT69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-72-a115b454f7da>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m  \u001b[0;34m'n_estimators'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m243\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m  'num_leaves': 28})\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0my_pred_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodelLGBM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_init_score, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[1;32m    893\u001b[0m             callbacks=None, init_model=None):\n\u001b[1;32m    894\u001b[0m         \u001b[0;34m\"\"\"Docstring is inherited from the LGBMModel.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         super().fit(X, y, sample_weight=sample_weight, init_score=init_score,\n\u001b[0m\u001b[1;32m    896\u001b[0m                     \u001b[0meval_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_sample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_sample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                     \u001b[0meval_init_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_init_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_metric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_metric\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[1;32m    746\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord_evaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    747\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 748\u001b[0;31m         self._Booster = train(\n\u001b[0m\u001b[1;32m    749\u001b[0m             \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m             \u001b[0mtrain_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightgbm/engine.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    290\u001b[0m                                     evaluation_result_list=None))\n\u001b[1;32m    291\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m         \u001b[0mbooster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0mevaluation_result_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, train_set, fobj)\u001b[0m\n\u001b[1;32m   3019\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__set_objective_to_none\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3020\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mLightGBMError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Cannot update due to null objective function.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3021\u001b[0;31m             _safe_call(_LIB.LGBM_BoosterUpdateOneIter(\n\u001b[0m\u001b[1;32m   3022\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3023\u001b[0m                 ctypes.byref(is_finished)))\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GradientBoostingRegressor"
      ],
      "metadata": {
        "id": "OigyqNprawf4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 원본데이터"
      ],
      "metadata": {
        "id": "_bBR3jKlidkr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "outputId": "e4782514-a352-4b0e-d9e8-93cc1f999ff4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1wclFDvgcg2e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BASELINE -- original features\n",
            "19242 6414\n",
            "Train RMSE: 19.37402533090049\n",
            "Test RMSE: 20.099227543729945\n",
            "Train, Test 차이 비율 0.03743167464908791\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "modelGBRegressor = GradientBoostingRegressor()\n",
        "\n",
        "print('BASELINE -- original features')\n",
        "print(len(X_train), len(X_val))\n",
        "modelGBRegressor.fit(X_train, y_train)\n",
        "\n",
        "y_pred_train = modelGBRegressor.predict(X_train)\n",
        "\n",
        "# 훈련 데이터에 대한 MSE 계산\n",
        "mse_train = mean_squared_error(y_train, y_pred_train)\n",
        "print(\"Train RMSE:\", np.sqrt(abs(mse_train)))\n",
        "\n",
        "# 적합된 모델을 사용하여 테스트 데이터에 대한 예측 수행\n",
        "y_pred_val = modelGBRegressor.predict(X_val)\n",
        "\n",
        "# 테스트 데이터에 대한 MSE 계산\n",
        "mse_val = mean_squared_error(y_val, y_pred_val)\n",
        "print(\"GradientBoostingRegressor VAL RMSE:\", np.sqrt(abs(mse_val)))\n",
        "print(\"GradientBoostingRegressor Train, VAL 차이 비율\", (np.sqrt(abs(mse_val))- np.sqrt(abs(mse_train)))/(np.sqrt(abs(mse_train))))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ORIGINAL DATA -  finding best parameter by using baseian method"
      ],
      "metadata": {
        "id": "aWl5o1Jfcg2q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import lightgbm as lgb\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from bayes_opt import BayesianOptimization\n",
        "\n",
        "# Define the loss function to be optimized\n",
        "def loss_function(learning_rate,n_estimators, subsample):\n",
        "    params = {\n",
        "        'loss': 'absolute_error',\n",
        "        'learning_rate': learning_rate,\n",
        "        'n_estimators': int(n_estimators),\n",
        "        'subsample' : subsample\n",
        "    }\n",
        "    # Train the LightGBM model with the specified parameters\n",
        "    model = GradientBoostingRegressor(**params)\n",
        "    model.fit(X_train, y_train)\n",
        "    \n",
        "    # Make predictions on the validation set\n",
        "    y_pred = model.predict(X_val)\n",
        "    \n",
        "    # Calculate the mean squared error (MSE)\n",
        "    mse = mean_squared_error(y_val, y_pred)\n",
        "    \n",
        "    # Return the negative MSE as the loss to be minimized\n",
        "    return -mse\n",
        "\n",
        "# Define the parameter ranges for the optimizer 파라미터 범위\n",
        "# 일반적으로 러닝레이트가 낮고, max\n",
        "pbounds = {'learning_rate': (0.01, 0.1),\n",
        "           'n_estimators' : (100, 300),\n",
        "           'subsample': (0.5,1)}\n",
        "\n",
        "# Create the Bayesian optimizer\n",
        "optimizer = BayesianOptimization(f=loss_function, pbounds=pbounds, random_state=42)\n",
        "\n",
        "# Run the optimization\n",
        "#init_point : 최적화를 진행하기 전 초기에 랜덤하게 점 뿌려주는 갯수,\n",
        "# 탐색과 활용을 균형있게 고려하기 위해 초기 점의 개수를 작은 값으로 시작하고 (예: 5-10),\n",
        "#  적절한 반복 횟수를 선택하는 것이 좋습니다 (예: 10-20) 조정\n",
        "optimizer.maximize(init_points=5, n_iter=10)\n",
        "\n",
        "# Get the best parameters and the best loss achieved\n",
        "best_params = optimizer.max['params']\n",
        "best_loss = -optimizer.max['target']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7806184-c74d-49c8-8dc3-ee8c16e14536",
        "id": "881HHHQHcg2q"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "|   iter    |  target   | learni... | n_esti... | subsample |\n",
            "-------------------------------------------------------------\n",
            "| \u001b[0m1        \u001b[0m | \u001b[0m-514.7   \u001b[0m | \u001b[0m0.04371  \u001b[0m | \u001b[0m290.1    \u001b[0m | \u001b[0m0.866    \u001b[0m |\n",
            "| \u001b[0m2        \u001b[0m | \u001b[0m-546.4   \u001b[0m | \u001b[0m0.06388  \u001b[0m | \u001b[0m131.2    \u001b[0m | \u001b[0m0.578    \u001b[0m |\n",
            "| \u001b[0m3        \u001b[0m | \u001b[0m-655.5   \u001b[0m | \u001b[0m0.01523  \u001b[0m | \u001b[0m273.2    \u001b[0m | \u001b[0m0.8006   \u001b[0m |\n",
            "| \u001b[0m4        \u001b[0m | \u001b[0m-549.5   \u001b[0m | \u001b[0m0.07373  \u001b[0m | \u001b[0m104.1    \u001b[0m | \u001b[0m0.985    \u001b[0m |\n",
            "| \u001b[0m5        \u001b[0m | \u001b[0m-529.0   \u001b[0m | \u001b[0m0.08492  \u001b[0m | \u001b[0m142.5    \u001b[0m | \u001b[0m0.5909   \u001b[0m |\n",
            "| \u001b[95m6        \u001b[0m | \u001b[95m-485.3   \u001b[0m | \u001b[95m0.08809  \u001b[0m | \u001b[95m295.1    \u001b[0m | \u001b[95m0.9762   \u001b[0m |\n",
            "| \u001b[0m7        \u001b[0m | \u001b[0m-504.6   \u001b[0m | \u001b[0m0.09368  \u001b[0m | \u001b[0m165.1    \u001b[0m | \u001b[0m0.5417   \u001b[0m |\n",
            "| \u001b[0m8        \u001b[0m | \u001b[0m-924.4   \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m183.6    \u001b[0m | \u001b[0m1.0      \u001b[0m |\n",
            "| \u001b[95m9        \u001b[0m | \u001b[95m-474.8   \u001b[0m | \u001b[95m0.1      \u001b[0m | \u001b[95m232.8    \u001b[0m | \u001b[95m0.5      \u001b[0m |\n",
            "| \u001b[0m10       \u001b[0m | \u001b[0m-515.5   \u001b[0m | \u001b[0m0.05608  \u001b[0m | \u001b[0m219.0    \u001b[0m | \u001b[0m0.9691   \u001b[0m |\n",
            "| \u001b[0m11       \u001b[0m | \u001b[0m-819.6   \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m247.3    \u001b[0m | \u001b[0m1.0      \u001b[0m |\n",
            "| \u001b[0m12       \u001b[0m | \u001b[0m-516.1   \u001b[0m | \u001b[0m0.08973  \u001b[0m | \u001b[0m154.7    \u001b[0m | \u001b[0m0.8899   \u001b[0m |\n",
            "| \u001b[0m13       \u001b[0m | \u001b[0m-1.072e+0\u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m117.4    \u001b[0m | \u001b[0m0.5      \u001b[0m |\n",
            "| \u001b[0m14       \u001b[0m | \u001b[0m-788.9   \u001b[0m | \u001b[0m0.01195  \u001b[0m | \u001b[0m226.2    \u001b[0m | \u001b[0m0.5253   \u001b[0m |\n",
            "| \u001b[0m15       \u001b[0m | \u001b[0m-482.0   \u001b[0m | \u001b[0m0.1      \u001b[0m | \u001b[0m236.0    \u001b[0m | \u001b[0m0.5      \u001b[0m |\n",
            "=============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SnVmH2mohnUr",
        "outputId": "73c6fb4e-2645-4c98-9183-0255af0fe5f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "474.7797391227841"
            ]
          },
          "metadata": {},
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 베스트 파라미터 Test SET 검증 ORIGINAL"
      ],
      "metadata": {
        "id": "qlJfPIlFcg2r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = GradientBoostingRegressor(**{'learning_rate': 0.1, 'n_estimators': 232, 'subsample': 0.5})\n",
        "model.fit(X_train, y_train)\n",
        "y_pred_train = model.predict(X_train)\n",
        "\n",
        "# 훈련 데이터에 대한 MSE 계산\n",
        "mse_train = mean_squared_error(y_train, y_pred_train)\n",
        "print(\"Train RMSE:\", np.sqrt(abs(mse_train)))\n",
        "\n",
        "# 적합된 모델을 사용하여 테스트 데이터에 대한 예측 수행\n",
        "y_pred_test = model.predict(X_test)\n",
        "\n",
        "# 테스트 데이터에 대한 MSE 계산\n",
        "mse_test = mean_squared_error(y_test, y_pred_test)\n",
        "print(\"GradientBoostingRegressor Test RMSE:\", np.sqrt(abs(mse_test)))\n",
        "print(\"GradientBoostingRegressor Train, Test 차이 비율\", (np.sqrt(abs(mse_test))- np.sqrt(abs(mse_train)))/(np.sqrt(abs(mse_train))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "560c84c1-2314-4ecc-8666-a48ae6c71d4e",
        "id": "nJD8ZiGacg2r"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train RMSE: 17.17284775890524\n",
            "GradientBoostingRegressor Test RMSE: 19.226090791316107\n",
            "GradientBoostingRegressor Train, Test 차이 비율 0.11956333982790505\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### POLYNOMIAL transformed data"
      ],
      "metadata": {
        "id": "_CbLjUvQhMhF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "poly = PolynomialFeatures(degree=2, include_bias=True)\n",
        "X_train_poly = poly.fit_transform(X_train)\n",
        "X_val_poly = poly.transform(X_val)"
      ],
      "metadata": {
        "id": "MQLTPaOYhLY0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "modelGBRegressor = GradientBoostingRegressor()\n",
        "\n",
        "print('BASELINE -- polynomial features')\n",
        "print(len(X_train_poly), len(X_val_poly))\n",
        "modelGBRegressor.fit(X_train_poly, y_train)\n",
        "\n",
        "y_pred_train = modelGBRegressor.predict(X_train_poly)\n",
        "\n",
        "# 훈련 데이터에 대한 MSE 계산\n",
        "mse_train = mean_squared_error(y_train, y_pred_train)\n",
        "print(\"Train RMSE:\", np.sqrt(abs(mse_train)))\n",
        "\n",
        "# 적합된 모델을 사용하여 테스트 데이터에 대한 예측 수행\n",
        "y_pred_val = modelGBRegressor.predict(X_val_poly)\n",
        "\n",
        "# 테스트 데이터에 대한 MSE 계산\n",
        "mse_val = mean_squared_error(y_val, y_pred_val)\n",
        "print(\"GradientBoostingRegressor POLYNOMIAL VAL RMSE:\", np.sqrt(abs(mse_val)))\n",
        "print(\"GradientBoostingRegressor POLYNOMIAL Train, VAL 차이 비율\", (np.sqrt(abs(mse_val))- np.sqrt(abs(mse_train)))/(np.sqrt(abs(mse_train))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        },
        "id": "_nsrrXQHhRVH",
        "outputId": "72c6c35d-dc15-411f-af52-bc107521d25a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BASELINE -- original features\n",
            "19242 6414\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-123-0fb9ded4252d>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'BASELINE -- original features'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_poly\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val_poly\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmodelGBRegressor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_poly\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0my_pred_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodelGBRegressor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_poly\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_gb.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m         \u001b[0;31m# fit the boosting stages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 538\u001b[0;31m         n_stages = self._fit_stages(\n\u001b[0m\u001b[1;32m    539\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_gb.py\u001b[0m in \u001b[0;36m_fit_stages\u001b[0;34m(self, X, y, raw_predictions, sample_weight, random_state, X_val, y_val, sample_weight_val, begin_at_stage, monitor)\u001b[0m\n\u001b[1;32m    613\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m             \u001b[0;31m# fit next stage of trees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 615\u001b[0;31m             raw_predictions = self._fit_stage(\n\u001b[0m\u001b[1;32m    616\u001b[0m                 \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_gb.py\u001b[0m in \u001b[0;36m_fit_stage\u001b[0;34m(self, i, X, y, raw_predictions, sample_weight, sample_mask, random_state, X_csc, X_csr)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_csr\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mX_csr\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m             \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresidual\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m             \u001b[0;31m# update tree leaves\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m   1245\u001b[0m         \"\"\"\n\u001b[1;32m   1246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1247\u001b[0;31m         super().fit(\n\u001b[0m\u001b[1;32m   1248\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1249\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m    377\u001b[0m             )\n\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "cbsKqWqwnp41",
        "ndnBHHOlv9uW",
        "EjdpSMbu_cy7",
        "nwNEgY25ea63",
        "NujHEy1uRYvJ",
        "OigyqNprawf4",
        "_bBR3jKlidkr",
        "aWl5o1Jfcg2q"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}